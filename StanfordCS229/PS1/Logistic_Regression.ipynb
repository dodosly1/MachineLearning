{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d2b334-f4e4-4fb0-acc7-037731a201bb",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "# Problem Set #1 (Unsupervised Learning)\n",
    "\n",
    "## 1. Logistic Regression [25 points]\n",
    "\n",
    "(a) [10 points] Consider the average empirical loss (the risk) for logistic regression:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} log(1+e^{y^{(i)} \\theta^{T} x^{(i)} }) = - \\frac{1}{m} \\sum_{i=1}^{m} log(h_{\\theta}(y^{(i)}x^{(i)}))$$\n",
    "\n",
    "where $ y^{(i)} \\in \\{ -1, 1\\}$, $h_{\\theta}(x) = g(\\theta^{T}x)$ and $g(z) = 1 /(1+e^{-z})$. Find\n",
    "the hessian of this function and show that for any vector $z$, it holds true that\n",
    "\n",
    "$$ z^{T} H z \\geq 0$$\n",
    "\n",
    "(a) [Solution]\n",
    "\n",
    "Before we proceed we define:\n",
    "* $m$ - number of training samples\n",
    "* $n$ - number of features\n",
    "* $x^{(i)}$ - ith training sample\n",
    "* $x^{(i)}_{j}$ j-th feature of the i-th training example\n",
    "\n",
    "Given the definition of a Hessian Matrix, we can write the Hessian Matrix of the average empirical loss $J(\\theta)$ as:\n",
    "\n",
    "$$(H_{J})_{i,j} = \\frac{\\partial^2 J}{\\partial \\theta_{i} \\partial \\theta_{j}}\\tag{1}$$\n",
    "\n",
    "We will start by computing the first partial derivative of $J(\\theta)$ with respect to $\\theta_{j}$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_{j}} =-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}[\\log(h_{\\theta}(y^{(i)}x^{(i)})]\\tag{2}$$\n",
    "\n",
    "Note that partial derivative is a linear operator, that why the partial derivative with respect to $\\theta_{j}$ is passed inside\n",
    "the sum in equation (2). Also it is important to note that $x^{(i)} \\in \\mathbb{R}^{n}$ and $y^{(i)} \\in [-1,1]$, thus their\n",
    "product is also an N-dimensional vector. \n",
    "\n",
    "Now we will substitute $h_{\\theta}(x^{(i)}y^{(i)})$ with $g(\\theta^{T}x^{(i)}y^{(i)})$ and thus equation (2) can be re-written\n",
    "as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_{j}} =-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}[\\log(g(\\theta^{T}y^{(i)}x^{(i)}))]\\tag{3}$$\n",
    "\n",
    "The partial derivative of $log(g(\\theta^{T}x^{(i)}y^{(i)}))$ with respect to $\\theta_{j}$ can be computed by using the chain\n",
    "rule:\n",
    "\n",
    "$$ \\frac{\\partial \\log(g)}{\\partial \\theta_{j}} = \\frac{1}{g} \\frac{\\partial g}{\\partial \\theta_{j}}\\tag{4}$$ \n",
    "\n",
    "We will know compute the partial derivative on the LHS of equation (4):\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_{j}} = \\frac{\\partial}{\\partial \\theta_{j}}\\left[\\frac{1}{1+e^{-\\theta^{T} x^{(i)}y^{(i)}}}\\right] $$\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial \\theta_{j}} = \\frac{0\\cdot(1+e^{-\\theta^{T}x^{(i)}y^{(i)}})- \\frac{\\partial}{\\partial \\theta_{j}} [1+e^{-\\theta^{T}x^{(i)}y^{(i)}}]}{(1+e^{-\\theta^{T}x^{(i)}y^{(i)}})^{2}} $$\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_{j}} = y^{(i)} \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T}x^{(i)}] \\frac{e^{\\theta^{T}x^{(i)}y^{(i)}}}{(1+e^{\\theta^{T}x^{(i)}y^{(i)}})^{2}} \\tag{5}$$\n",
    "\n",
    "Equation (5) seems a little complicated. In order to obtain a more helpful representation of equation (5), we will re-write some\n",
    "of its terms.\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T} x^{(i)} ] =\\frac{\\partial}{\\partial \\theta_{j}}[\\sum_{l=0}^{n}\\theta_{l}x^{(i)}_{l}] =\\frac{\\partial}{\\partial \\theta_{j}}[\\theta_{0}x^{(i)}_{0}+\\theta_{1}x^{(i)}_1+..+\\theta_{n}x^{(i)}_{n}] \\tag{6}$$\n",
    "\n",
    "In equation (6), all the derivatives will be zero when $j \\neq l$. In case that $j=l$ the derivative will be simply equal to:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T}x^{(i)}] = x^{(i)}_{j} \\tag{7}$$\n",
    "\n",
    "Also the last term in the LHS of equation (5) can be written as:\n",
    "\n",
    "$$\\frac{e^{\\theta^{T}x^{(i)}y^{(i)}}}{(1+e^{\\theta^{T}x^{(i)}y^{(i)}})^{2}} = g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{8}$$\n",
    "\n",
    "By substituting equations (7) and (8) into equation (5), we get\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial \\theta_{j}} = y^{(i)} x^{(i)}_{j} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{9}$$\n",
    "\n",
    "\n",
    "And by substituting equation (9) into equation (4) we simply get:\n",
    "$$ \\frac{\\partial \\log(g)}{\\partial \\theta_{j}} = y^{(i)} x^{(i)}_{j} (1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{10} $$\n",
    "\n",
    "\n",
    "Finally, by substituting equation (10) into equation (4) we find that the partial derivative of $J$ with respect to $\\theta_{j}$ is\n",
    "equal to:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_{j}} = - \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} x^{(i)}_{j} (1-g(\\theta^{T}x^{(i)}y^{(i)})) $$\n",
    "\n",
    "We will continue by computing the second spatial derivative with respect to $\\theta_{i}$:\n",
    "\n",
    "(b) We have provided two data files:\n",
    "*  http://cs229.stanford.edu/ps/ps1/logistic_x.txt\n",
    "*  http://cs229.stanford.edu/ps/ps1/logistic_y.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4750f-eed8-4418-a673-d01290e4af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc601d-29f4-4212-8541-838110253d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
