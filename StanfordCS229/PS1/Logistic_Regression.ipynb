{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d2b334-f4e4-4fb0-acc7-037731a201bb",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "# Problem Set #1 (Unsupervised Learning)\n",
    "\n",
    "## 1. Logistic Regression [25 points]\n",
    "\n",
    "(a) [10 points] Consider the average empirical loss (the risk) for logistic regression:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} log(1+e^{y^{(i)} \\theta^{T} x^{(i)} }) = - \\frac{1}{m} \\sum_{i=1}^{m} log(h_{\\theta}(y^{(i)}x^{(i)}))$$\n",
    "\n",
    "where $ y^{(i)} \\in \\{ -1, 1\\}$, $h_{\\theta}(x) = g(\\theta^{T}x)$ and $g(z) = 1 /(1+e^{-z})$. Find\n",
    "the hessian of this function and show that for any vector $z$, it holds true that\n",
    "\n",
    "$$ z^{T} H z \\geq 0$$\n",
    "\n",
    "(a) [Solution]\n",
    "\n",
    "Before we proceed we define:\n",
    "* $m$ - number of training samples\n",
    "* $n$ - number of features\n",
    "* $x^{(i)}$ - ith training sample\n",
    "* $x^{(i)}_{j}$ j-th feature of the i-th training example\n",
    "\n",
    "Given the definition of a Hessian Matrix, we can write the Hessian Matrix of the average empirical loss $J(\\theta)$ as:\n",
    "\n",
    "$$(H_{J})_{i,j} = \\frac{\\partial^2 J}{\\partial \\theta_{i} \\partial \\theta_{j}}\\tag{1}$$\n",
    "\n",
    "We will start by computing the first partial derivative of $J(\\theta)$ with respect to $\\theta_{j}$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_{j}} =-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}[\\log(h_{\\theta}(y^{(i)}x^{(i)})]\\tag{2}$$\n",
    "\n",
    "Note that partial derivative is a linear operator, that why the partial derivative with respect to $\\theta_{j}$ is passed inside\n",
    "the sum in equation (2). Also it is important to note that $x^{(i)} \\in \\mathbb{R}^{n}$ and $y^{(i)} \\in [-1,1]$, thus their\n",
    "product is also an N-dimensional vector. \n",
    "\n",
    "Now we will substitute $h_{\\theta}(x^{(i)}y^{(i)})$ with $g(\\theta^{T}x^{(i)}y^{(i)})$ and thus equation (2) can be re-written\n",
    "as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_{j}} =-\\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\theta_{j}}[\\log(g(\\theta^{T}y^{(i)}x^{(i)}))]\\tag{3}$$\n",
    "\n",
    "The partial derivative of $log(g(\\theta^{T}x^{(i)}y^{(i)}))$ with respect to $\\theta_{j}$ can be computed by using the chain\n",
    "rule:\n",
    "\n",
    "$$ \\frac{\\partial \\log(g)}{\\partial \\theta_{j}} = \\frac{1}{g} \\frac{\\partial g}{\\partial \\theta_{j}}\\tag{4}$$ \n",
    "\n",
    "We will know compute the partial derivative on the LHS of equation (4):\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_{j}} = \\frac{\\partial}{\\partial \\theta_{j}}\\left[\\frac{1}{1+e^{-\\theta^{T} x^{(i)}y^{(i)}}}\\right] $$\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial \\theta_{j}} = \\frac{0\\cdot(1+e^{-\\theta^{T}x^{(i)}y^{(i)}})- \\frac{\\partial}{\\partial \\theta_{j}} [1+e^{-\\theta^{T}x^{(i)}y^{(i)}}]}{(1+e^{-\\theta^{T}x^{(i)}y^{(i)}})^{2}} $$\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_{j}} = y^{(i)} \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T}x^{(i)}] \\frac{e^{\\theta^{T}x^{(i)}y^{(i)}}}{(1+e^{\\theta^{T}x^{(i)}y^{(i)}})^{2}} \\tag{5}$$\n",
    "\n",
    "Equation (5) seems a little complicated. In order to obtain a more helpful representation of equation (5), we will re-write some\n",
    "of its terms.\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T} x^{(i)} ] =\\frac{\\partial}{\\partial \\theta_{j}}[\\sum_{l=0}^{n}\\theta_{l}x^{(i)}_{l}] =\\frac{\\partial}{\\partial \\theta_{j}}[\\theta_{0}x^{(i)}_{0}+\\theta_{1}x^{(i)}_1+..+\\theta_{n}x^{(i)}_{n}] \\tag{6}$$\n",
    "\n",
    "In equation (6), all the derivatives will be zero when $j \\neq l$. In case that $j=l$ the derivative will be simply equal to:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}}[\\theta^{T}x^{(i)}] = x^{(i)}_{j} \\tag{7}$$\n",
    "\n",
    "Also the last term in the LHS of equation (5) can be written as:\n",
    "\n",
    "$$\\frac{e^{\\theta^{T}x^{(i)}y^{(i)}}}{(1+e^{\\theta^{T}x^{(i)}y^{(i)}})^{2}} = g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{8}$$\n",
    "\n",
    "By substituting equations (7) and (8) into equation (5), we get\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial \\theta_{j}} = y^{(i)} x^{(i)}_{j} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{9}$$\n",
    "\n",
    "\n",
    "And by substituting equation (9) into equation (4) we simply get:\n",
    "$$ \\frac{\\partial \\log(g)}{\\partial \\theta_{j}} = y^{(i)} x^{(i)}_{j} (1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{10} $$\n",
    "\n",
    "\n",
    "Finally, by substituting equation (10) into equation (4) we find that the partial derivative of $J$ with respect to $\\theta_{j}$ is\n",
    "equal to:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta_{j}} = - \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} x^{(i)}_{j} (1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{11}$$\n",
    "\n",
    "We will continue by computing the second spatial derivative with respect to $\\theta_{k}$. We introduce a new variable k in order to\n",
    "not confuse it with $i$ used for summing across all $m$ training examples. Thus,\n",
    "\n",
    "$$ \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)}x^{(i)}_{j} \\frac{\\partial g}{\\partial \\theta_{k}} \\tag{12} $$\n",
    "\n",
    "We had previously computed the derivative of $ \\frac{\\partial J}{\\partial \\theta_{j}}$ (equation (9)), the equation will be exactly the sum but instead of $ x_{j}^{(i)}$ we we have the k-th feature of training sample $x^{(i)}$. Thus,\n",
    "\n",
    "$$  \\frac{\\partial g}{\\partial \\theta_{j}} = y^{(i)} x^{(i)}_{k} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{13} $$\n",
    "\n",
    "Now, we will substitute equation (13) into equation (12)\n",
    "\n",
    "$$ \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)}x^{(i)}_{j} y^{(i)} x^{(i)}_{k} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)}))$$\n",
    "\n",
    "With simple algebric manipulation it can be re-written as:\n",
    "$$  \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)})^{2} x^{(i)}_{j} x^{(i)}_{k} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{14} $$\n",
    "\n",
    "Now we will compute:\n",
    "\n",
    "$$ \n",
    "  z^{T} H z = \\begin{bmatrix} z_{0} & z_{1} & \\cdots & z_{n}\\end{bmatrix} \\begin{bmatrix}\\frac{\\partial^2 J}{\\partial \\theta_{0} \\partial \\theta_{0}} & \\cdots & \\frac{\\partial^2 J}{\\partial \\theta_{0} \\partial \\theta_{n-1}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 J}{\\partial \\theta_{n-1} \\partial \\theta_{0}} & \\cdots & \\frac{\\partial^2 J}{\\partial \\theta_{n-1} \\partial \\theta_{n-1}} \\end{bmatrix} \\begin{bmatrix} z_{0} \\\\ z_{1} \\\\ \\vdots \\\\ z_{n} \\end{bmatrix} \\tag{15}\n",
    "$$\n",
    "\n",
    "Vector's $z^{T}$ dimenions are $(1,n)$ and the dimensions of the hessian matrix are $(n,n)$. The two inner dimensions are of the same \n",
    "size and their product will be of size $(1,n)$\n",
    "\n",
    "$$ z^{T}H = \\begin{bmatrix} \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{0}} &  \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{1}} & \\cdots & \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{n-1}} \\end{bmatrix} \\tag{16}$$\n",
    "\n",
    "We can substitute the RHS of equation (16) into equation(15) in order to determine $z^{T}H z$.\n",
    "\n",
    "$$(z^{T}H) \\cdot z = \\begin{bmatrix} \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{0}} &  \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{1}} & \\cdots & \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{n-1}} \\end{bmatrix} \\begin{bmatrix} z_{0} \\\\ z_{1} \\\\ \\vdots \\\\ z_{n} \\end{bmatrix}$$\n",
    "\n",
    "Which can be equivalently written as: \n",
    "\n",
    "$$ \n",
    "z^{T}H z = z_{0} \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{0}} + z_{1} \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{1}} + \\cdots + z_{n} \\sum_{k=0}^{n-1} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{n}}\n",
    "$$\n",
    "\n",
    "Which can be further simplified to:\n",
    "\n",
    "$$\n",
    "z^{T}H z = \\sum_{j=0}^{n-1} \\sum_{k=0}^{k-1} z_{j} z_{k} \\frac{\\partial^2 J}{\\partial \\theta_{k} \\partial \\theta_{j}} \\tag{17}\n",
    "$$\n",
    "\n",
    "We can replace the second partial derivative of the cost function $J$ with respect to $\\theta_{k}$ and $\\theta_{j}$ from equation (15)\n",
    "into equation (17), thus\n",
    "\n",
    "$$\n",
    "z^{T}H z = \\sum_{j=0}^{n-1} \\sum_{k=0}^{k-1} z_{j} z_{k}  \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)})^{2} x^{(i)}_{j} x^{(i)}_{k} g(\\theta^{T}x^{(i)}y^{(i)})(1-g(\\theta^{T}x^{(i)}y^{(i)})) \\tag{18}\n",
    "$$\n",
    "\n",
    "We can re-write equation (18), more coveniently:\n",
    "$$\n",
    "z^{T}H z =  \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)})^{2}\\left(\\sum_{k=0}^{n-1} \\sum_{j=0}^{n-1} z_{j} z_{k} x^{(i)}_{j} x^{(i)}_{k}\\right)  h_{\\theta}(x^{(i)}y^{(i)})(1-h_{\\theta}(x^{(i)}y^{(i)})) \\tag{19}\n",
    "$$\n",
    "\n",
    "We have that $y^{(i)} \\in {-1,1} $, thus $(y^{(i)})^2 \\geq 0$.\n",
    "\n",
    "We also have that:\n",
    "\n",
    "$$ \\sum_{k=0}^{n-1} \\sum_{j=0}^{n-1} z_{j} z_{k} x^{(i)}_{j} x^{(i)}_{k}= \\sum_{k=0}^{n-1} z_{k} x^{(i)}_{k} \\sum_{j=0}^{n-1} z_{j} x^{(i)}_{j} = (z^{T}x)(z^{T}x) = (z^{T}x)^{2} \\geq 0\n",
    "$$\n",
    "\n",
    "Finally, our hypothesis is the sigmoid function,thus we know that $h_{\\theta}(x^{(i)}y^{(i)}) \\in (0,1)$ and $1-h_{\\theta}(x^{(i)}y^{(i)}) \\in (0,1)$. Thus, we can conclude:\n",
    "\n",
    "$$\n",
    " z^{T} H z \\geq 0\n",
    "$$\n",
    "\n",
    "(b) We have provided two data files:\n",
    "\n",
    "*  http://cs229.stanford.edu/ps/ps1/logistic_x.txt\n",
    "*  http://cs229.stanford.edu/ps/ps1/logistic_y.txt\n",
    "\n",
    "These files contain the inputs $(x^{(i)} \\in \\mathbb{R}^{2})$ and outputs $(y^{(i)}) \\in \\{-1.1\\})$, respectively for a binary classification problem, with one training example per row. Implement Newton's\n",
    "method for optimizing $J(\\theta)$, and apply it to fit a logistic regression model to the data. Initialize\n",
    "Newton's method with $\\theta = \\vec{0}$. What are the coefficients $\\theta$ resulting from your fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04181630-35b4-413e-9422-3c6368d3ea61",
   "metadata": {},
   "source": [
    "## Importing the Required Libraries\n",
    "\n",
    "First, we will have to import all the libraries that we will use in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b4750f-eed8-4418-a673-d01290e4af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Python's Numerical Library\n",
    "import matplotlib.pyplot as plt # Tool for data visualization in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc52cc-6191-40dc-9c27-427944ace129",
   "metadata": {},
   "source": [
    "## Appending 1s to each training sample\n",
    "\n",
    "We want puy hypothesis to be:\n",
    "\n",
    "$$h_{\\theta}(x^{(i)}) = g(\\theta^Tx^{(i)})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ z^{(i)} = \\theta^{T} x^{(i)} = \\theta_{0} x^{(i)}_{0} + \\theta_{1}x^{(i)}_{1}+\\theta_{2}x^{(i)}_{2}$$\n",
    "\n",
    "with $x^{(i)}_{0} = 1$ for all training examples\n",
    "\n",
    "We would have to add a dummy feature in each one of our trainings samples in order to include the intercept term $\\theta_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1de675-7c27-44fc-bcf4-0609ca84f4e8",
   "metadata": {},
   "source": [
    "## Implementing Newtons method\n",
    "\n",
    "### Logistic Regression and its objective function\n",
    "\n",
    "In logistic regression, we assume that (propabillity mass function):\n",
    "\n",
    "$$ p(y^{(i)} = 1 | x^{(i)} ; \\theta) = h_{\\theta}(x^{(i)}) \\tag{1}$$\n",
    "\n",
    "$$ p(y^{(i)} = 0 | x^{(i)} ; \\theta) = 1-h_{\\theta}(x^{(i)}) \\tag{2}$$\n",
    "\n",
    "Equations (1) and (2) describe the probability that the output y equals to 1 (or -1) given a feature\n",
    "vector $x$ and the hypothesis function's weights $\\theta$\n",
    "\n",
    "Equations (1) and (2) can be condensed  into a single equation.\n",
    "\n",
    "$$ p(y^{(i)}|x^{(i)};\\theta) = (h_{\\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{(1-y^{(i)})} \\tag{3}$$\n",
    "\n",
    "We want to generalise (3) across all training examples. Assuming that\n",
    "the training examples $m$ where sampled independently we could write the likelihood\n",
    "of the parameters as:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\prod_{i=1}^{m} p(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^{m} (h_{\\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{(1-y^{(i)})} \\tag{4}$$\n",
    "\n",
    "A propabillity is a number within $[0,1]$. As a cosquence the product of the \n",
    "propabillities of all $m$ training examples will be of magnitude $10^{-m}$. \n",
    "Our objective function also penalizes our model by generating small values.\n",
    "The precision provided by numpy may not be sufficient and as result it would\n",
    "round up all small numbers to zero.\n",
    "\n",
    "Instead of maximizing the likelihood function we will attempt to maximize the log \n",
    "likelihood function:\n",
    "\n",
    "$$ \\mathcal{l}(\\theta) = \\sum_{i=1}^{m} y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)})) \\tag{5}$$\n",
    "\n",
    "In addition, the log-likelihood function is strictly concave, which means that it exhibits one global optima.\n",
    "\n",
    "### Newton's method\n",
    "\n",
    "We want to maximize $\\mathcal{l}(\\theta)$. Since our problem will have one global optima, this is similar to\n",
    "finding $\\theta$ such that $\\mathcal{l}'(\\theta) = 0$\n",
    "\n",
    "We can do this by using Netwon's method to search for the optimum value of $\\theta$.\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - H^{-1} \\nabla_{\\theta} \\mathcal{l}(\\theta)$$\n",
    "\n",
    "where $H$ is the hessian matrix of $\\mathcal{l}(\\theta)$ and $\\nabla_{\\theta} \\mathcal{l}(\\theta)$ are the first order partial derivatives of $\\mathcal{l}(\\theta)$.\n",
    "\n",
    "$$ \\nabla_{\\theta} \\mathcal{l}(\\theta) = \\begin{bmatrix} \\sum_{i=1}^{m}(y^{(i)}-h_{\\theta}(x^{(i)})) \\\\ \\sum_{i=1}^{m}(y^{(i)}-h_{\\theta}(x^{(i)})) x^{(i)}_{1} \\\\ \\sum_{i=1}^{m}(y^{(i)}-h_{\\theta}(x^{(i)})) x^{(i)}_{2}\\end{bmatrix}$$\n",
    "\n",
    "$$ H_{k,j} = \\sum_{i=1}^{m} h_{\\theta}(x^{(i)})(1-h_{\\theta}(x^{(i)})) x^{(i)}_{j} x^{(i)}_{k} $$\n",
    "\n",
    "The estimation of $\\theta$ is perfomed iteratevely until $\\theta_{n+1} - \\theta_{n} \\approx 0$\n",
    "\n",
    "### What we have to implement?\n",
    "\n",
    "In order to implement newtons method on logistic regression we have to:\n",
    "\n",
    "** Declare our hypothesis function\n",
    "** Declare our objective function \n",
    "** Implement the update rule\n",
    "** Implement the Newtons method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77db37c1-cb28-4601-97db-3ce114aacccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis function \n",
    "\n",
    "# Input : m-training samples \n",
    "# Output: a single value for each training example \n",
    "\n",
    "def h(x,th):\n",
    "    #Array to store the linear part of the logistic regression\n",
    "    z = np.zeros(x.shape[0])\n",
    "    \n",
    "    #Calculate row-wise the dot product between x and hypothesis parameters theta\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = np.dot(th,x[i,:])\n",
    "        \n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785f7fe2-5fef-436c-ad57-53ae43b123e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "\n",
    "# Input: An array x of m training examples, \n",
    "#        an array with the class of each training examples, \n",
    "#        the hypothesis parameters theta\n",
    "# Output: the log_likehood of the model\n",
    "\n",
    "def log_likehood(x,y,th):\n",
    "    sigmoid_probs = h(x,th)\n",
    "    l_th = np.sum(y*np.log(sigmoid_probs) + (1-y)*np.log(1-sigmoid_probs))\n",
    "    return l_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511cb7b4-d7f0-4ae9-b147-88216e599ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the objective function\n",
    "\n",
    "# Input: \n",
    "#       an array x of m training samples\n",
    "#       an array y with the class of each training example\n",
    "#       the hypothesis parameters theta\n",
    "# Output:\n",
    "#       a vector with dimensions (1,n)\n",
    "#\n",
    "\n",
    "def gradient(x,y,th):\n",
    "    print(th.shape)\n",
    "    sigmoid_probs = h(x,th)\n",
    "    grad = np.zeros(th.shape[0])\n",
    "    \n",
    "    constant = y-sigmoid_probs\n",
    "\n",
    "    for i in range(th.shape[0]):\n",
    "        grad[i] = np.sum(constant*x[:,i])\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd299d99-63d4-4588-a5f1-af1547a4b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hessian of the objective function\n",
    "\n",
    "def Hessian(x,y,th):\n",
    "    H = np.zeros((th.shape[0],th.shape[0]))\n",
    "    sigmoid_probs = h(x,th)\n",
    "    constant = np.multiply(sigmoid_probs,(1-sigmoid_probs)) \n",
    "    print(constant)\n",
    "    for j in range(th.shape[0]):\n",
    "        for k in range(th.shape[0]):\n",
    "            H[j,k] = np.sum(constant*x[:,j]*x[:,k])\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf55c8bf-2305-4db7-a0cb-086fe395613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hypothesis parameters \n",
    "theta = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8892f9-42b7-4ca2-b2e3-e3353fab3416",
   "metadata": {},
   "source": [
    "## Reading Data from txt values\n",
    "\n",
    "In order to read numerical data from txt we will use numpy's built in function loadtxt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9cc601d-29f4-4212-8541-838110253d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the class of each training sample\n",
    "y_train = np.loadtxt('logistic_y.txt')\n",
    "#Load the features of each training sample\n",
    "x_train = np.loadtxt('logistic_x.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a68f973f-ad46-4bc3-a62d-df890d691930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dummy feature x0 = 1 in all training examples\n",
    "x_train = np.hstack((x_train,np.ones((x_train.shape[0],1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7110e-3db5-493f-8b06-7a8a37cca31c",
   "metadata": {},
   "source": [
    "### Implementing Newtons Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0387ca98-3d7f-40f1-a5de-346fb10ccfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25\n",
      " 0.25]\n",
      "-68.62157087543459\n",
      "(3,)\n",
      "[0.00403065 0.01421386 0.00148505 0.00465266 0.00285708 0.00281264\n",
      " 0.00292544 0.0018652  0.00070045 0.0010043  0.00138314 0.00104906\n",
      " 0.00056951 0.00158415 0.0024678  0.00334328 0.01425136 0.01500216\n",
      " 0.00636647 0.00739003 0.00751573 0.01907109 0.01258506 0.00795913\n",
      " 0.01256481 0.04530288 0.04195577 0.07426679 0.0433178  0.01720239\n",
      " 0.00707791 0.01020305 0.00880812 0.00418175 0.00236549 0.21971164\n",
      " 0.23570455 0.21477622 0.02420242 0.00396917 0.02151643 0.03807645\n",
      " 0.05411135 0.06627458 0.11011299 0.10134639 0.06647868 0.04583246\n",
      " 0.22290177 0.0513931  0.19799066 0.17542558 0.24165853 0.23898847\n",
      " 0.24335141 0.24238396 0.24649393 0.23447106 0.22174671 0.24967447\n",
      " 0.22650901 0.17207325 0.24072059 0.05540586 0.02398902 0.02969639\n",
      " 0.06316466 0.01921916 0.24834797 0.24701719 0.21293304 0.24649347\n",
      " 0.24308455 0.02707841 0.02471417 0.21745963 0.24937102 0.14730465\n",
      " 0.15553835 0.19589558 0.12773275 0.06712759 0.18352562 0.00523753\n",
      " 0.01279621 0.05189773 0.19424698 0.06565142 0.24894745 0.24937216\n",
      " 0.22428574 0.12470921 0.04815557 0.01897971 0.21859262 0.21337697\n",
      " 0.23027676 0.18832072 0.14737104]\n",
      "-488.87943959940145\n",
      "(3,)\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.17381907e-10\n",
      " 5.61996249e-04 9.57983965e-07 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.15846864e-06 0.00000000e+00 6.22305283e-09 1.77807553e-11\n",
      " 1.07464638e-02 2.63367012e-03 7.16419344e-06 4.98629633e-06\n",
      " 7.94422822e-05 1.63619751e-07 1.29837797e-05 1.34715168e-03\n",
      " 5.20667867e-05 1.90414015e-11 1.45754845e-02 4.93514999e-27\n",
      " 3.09691697e-36 3.46198143e-34 1.85257038e-25 9.51800922e-39\n",
      " 2.14859687e-01 4.51060078e-05 6.49312309e-07 2.19258754e-01\n",
      " 3.51918746e-02 2.80683296e-35 2.46754596e-36 3.32914757e-06\n",
      " 1.42220032e-03 2.36257780e-14 1.16238479e-13 1.83538289e-09\n",
      " 7.54365177e-17 3.61726590e-25 5.88902984e-11 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.38078528e-10 0.00000000e+00\n",
      " 2.40672372e-04 4.24332927e-04 5.34811084e-10 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.03518099e-10 3.95932176e-11\n",
      " 4.61830074e-08 2.16937579e-13 0.00000000e+00]\n",
      "nan\n",
      "(3,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35511/3810579500.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  l_th = np.sum(y*np.log(sigmoid_probs) + (1-y)*np.log(1-sigmoid_probs))\n",
      "/tmp/ipykernel_35511/3810579500.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  l_th = np.sum(y*np.log(sigmoid_probs) + (1-y)*np.log(1-sigmoid_probs))\n",
      "/tmp/ipykernel_35511/4198629969.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m g \u001b[38;5;241m=\u001b[39m gradient(x_train,y_train,theta)\n\u001b[1;32m      7\u001b[0m H1 \u001b[38;5;241m=\u001b[39m Hessian(x_train,y_train,theta)\n\u001b[0;32m----> 9\u001b[0m H_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m delta_theta \u001b[38;5;241m=\u001b[39m H_inv \u001b[38;5;241m@\u001b[39m g\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_likehood(x_train,y_train,theta))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "##\n",
    "delta_theta = np.array([0.5,0.5,0.5])\n",
    "\n",
    "while np.max(delta_theta) > 0.001:\n",
    "    \n",
    "    g = gradient(x_train,y_train,theta)\n",
    "    H1 = Hessian(x_train,y_train,theta)\n",
    "    \n",
    "    H_inv = np.linalg.inv(H1)\n",
    "    \n",
    "    delta_theta = H_inv @ g\n",
    "\n",
    "    print(log_likehood(x_train,y_train,theta))\n",
    "    theta = theta - delta_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43fa25-3195-419d-ab8c-43429cedff1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34b785-cf2d-441a-8bdb-475f46fd8209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
